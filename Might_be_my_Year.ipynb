{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpK4IHpoHv0oai0s9ySAIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JimMiller-0/Might-be-my-Year/blob/main/Might_be_my_Year.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started"
      ],
      "metadata": {
        "id": "-grNmp9GD6B8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a notebook for retreiving Fantasy Football Data from ESPN for analysis and predictive capabilities"
      ],
      "metadata": {
        "id": "z3kfLMqUCIae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Necessary Librarys & SDKs"
      ],
      "metadata": {
        "id": "16MbXDTu05eO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AFYH1buE0yXl"
      },
      "outputs": [],
      "source": [
        "# Install necessary SDKs\n",
        "\n",
        "!pip install google-cloud-secret-manager\n",
        "!pip install google-cloud-bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract Data from ESPN private Fantasy Football League"
      ],
      "metadata": {
        "id": "l--70pQMEHDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authenticate to ESPN Fantasy Football API\n",
        "\n",
        "You'll need a couple things in order to authenticate to the ESPN Fantasy Football API. first retrieve and set league variables like league ID and season. Next you'll have to go through the process of getting the ESPN s2 and SWID access tokens. These access tokens we'll want to keep safe in GCP secrets manager.\n",
        "\n",
        "\n",
        "Follow these instructions to get espn_s2 and swid: https://github.com/cwendt94/espn-api/discussions/150 or this blog: https://jman4190.medium.com/how-to-use-python-with-the-espn-fantasy-draft-api-ecde38621b1b\n",
        "\n",
        "\n",
        "Follow these instructions to create secrets in GCP Secrets Manager: https://cloud.google.com/secret-manager/docs/creating-and-accessing-secrets#secretmanager-create-secret-console\n",
        "\n",
        "Once the variables are set and the access tokens are secured in GCP secrets manager, go ahead and execute the code below. it will go retrieve the access tokens and set all the variable to use in each request going forward."
      ],
      "metadata": {
        "id": "2bIPDfui5_1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import secretmanager\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "# You will need to get your league ID and ESPN S2 and SWID\n",
        "# See https://github.com/cwendt94/espn-api/wiki/Football-Intro for details\n",
        "# Recommended: Store SWID and ESPN S2 in a secrets manager, like gcp secrets manager: https://cloud.google.com/security/products/secret-manager\n",
        "\n",
        "league_id = 1054374 # => set to league ID that you want to pull data from\n",
        "season=2023 # => set to year you want to pull data from\n",
        "url=f'https://lm-api-reads.fantasy.espn.com/apis/v3/games/ffl/seasons/{season}/segments/0/leagues/{league_id}?scoringPeriodId=17&view=mBoxscore&view=m' # => url of ESPN API. Note: this has changed over the years, navigate to the site and inspect network calls to get current endpoint\n",
        "\n",
        "# Authenticate to Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#Create a Client for secrets manager\n",
        "client = secretmanager.SecretManagerServiceClient()\n",
        "project_id = 'might-be-my-year' # => GCP Project ID where secrets manager is enabled\n",
        "secret_espn_s2 = 'espn_s2' # name of secret in GCP secrets manager for espn_s2\n",
        "secret_swid = 'swid' # name of secret in GCP secrets manager for swid\n",
        "\n",
        "# Forge the paths to the latest version of the secrets with a F-string:\n",
        "resource_name_espn_s2 = f\"projects/{project_id}/secrets/{secret_espn_s2}/versions/latest\"\n",
        "resource_name_swid = f\"projects/{project_id}/secrets/{secret_swid}/versions/latest\"\n",
        "\n",
        "# Load up the secrets to a variable at runtime:\n",
        "response_espn_s2 = client.access_secret_version(name=resource_name_espn_s2)\n",
        "response_swid = client.access_secret_version(name=resource_name_swid)\n",
        "\n",
        "espn_s2 = response_espn_s2.payload.data.decode(\"UTF-8\")\n",
        "swid = response_swid.payload.data.decode(\"UTF-8\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ItuWEaB354yR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data From ESPN API\n",
        "\n",
        "We loop through a series of URLs representing the API endpoints for particular classes:\n",
        "\n",
        "\n",
        "*   Draft\n",
        "*   Free agency (weekly)\n",
        "*   Players\n",
        "*   Teams - Fantasy Football (not nfl)\n",
        "*   Members\n",
        "*   Player Weekly Box Score\n",
        "*   Team Weekly Box Score\n",
        "*   Matchups - Weekly Team vs Team Stats\n",
        "*   League Settings\n",
        "\n",
        "We save the raw data from the API to json file on our file system. these files aren't in a format to be loaded into BigQuery. In our next step we'll transform them so they are ready to be loaded into our BQ instance\n",
        "\n"
      ],
      "metadata": {
        "id": "YePLEaBR6cgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For each season we want to query the ESPN FF API and bring back the raw data and save to a json file with an apporpriate name (i.e draft_2023.json) {class_name}_{season}.json\n",
        "\n",
        "for season in range(2018, 2024):\n",
        "  base_url=f'https://lm-api-reads.fantasy.espn.com/apis/v3/games/ffl/'\n",
        "  auth_cookies={\"swid\": swid, \"espn_s2\": espn_s2}\n",
        "\n",
        "# we need to check the API route based on season/year. As the ESPN API updated over the years there were differences with how to interact with the API.\n",
        "  if season >= 2018:\n",
        "    endpoint = f'{base_url}seasons/{season}/segments/0/leagues/{league_id}'\n",
        "    print(f'Getting data for year: {season}')\n",
        "  else:\n",
        "    ednppoint = f'{base_url}leagueHistory/{league_id}?seasonId={season}'\n",
        "    print(f'Getting historical data for year: {season}')\n",
        "\n",
        "# build the endpoint for each class, -> call the endpoint to get raw data -> sotre data in json file. Each class is a little different.\n",
        "\n",
        "\n",
        "#Matchup - Weekly\n",
        "  for week in range(1,18):\n",
        "\n",
        "    matchup_filename = f\"matchups_{season}_week_{week}.json\"\n",
        "\n",
        "    if os.path.exists(matchup_filename):\n",
        "      os.remove(matchup_filename)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    r = requests.get(f'{endpoint}?view=mMatchup&view=mMatchupScore&scoringPeriodId={week}', cookies=auth_cookies)\n",
        "    json_object = json.dumps(r.json(), indent = 4)\n",
        "    with open(matchup_filename, \"w\") as outfile:\n",
        "      outfile.write(json_object)\n",
        "    time.sleep(0.08)\n",
        "\n",
        "#Activity - Weekly - gets add/drops with bid amount for successful and failed bids along with what the team had left with in their FA budget. Also Trades- probably can be cleaned up but whatever\n",
        "  for week in range(1,18):\n",
        "\n",
        "    transactions_filename = f\"transactions_{season}_week_{week}.json\"\n",
        "\n",
        "    if os.path.exists(transactions_filename):\n",
        "      os.remove(transactions_filename)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    r = requests.get(f'{endpoint}?scoringPeriodId={week}&view=mDraftDetail&view=mStatus&view=mSettings&view=mTeam&view=mTransactions2&view=modular&view=mNav', cookies=auth_cookies)\n",
        "    json_object = json.dumps(r.json(), indent = 4)\n",
        "    with open(transactions_filename, \"w\") as outfile:\n",
        "      outfile.write(json_object)\n",
        "    time.sleep(0.08)\n",
        "\n",
        "#Rosters -weekly\n",
        "  for week in range(1,18):\n",
        "\n",
        "    roster_filename = f\"roster_{season}_week_{week}.json\"\n",
        "\n",
        "    if os.path.exists(roster_filename):\n",
        "      os.remove(roster_filename)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    r = requests.get(f'{endpoint}?scoringPeriodId={week}&view=mRoster&view=mTeam', cookies=auth_cookies)\n",
        "    json_object = json.dumps(r.json(), indent = 4)\n",
        "    with open(roster_filename, \"w\") as outfile:\n",
        "      outfile.write(json_object)\n",
        "    time.sleep(0.08)\n",
        "\n",
        "#NFL Schedule\n",
        "  nfl_schedule_filename = f\"nfl_schedule_{season}.json\"\n",
        "\n",
        "  if os.path.exists(nfl_schedule_filename):\n",
        "    os.remove(nfl_schedule_filename)\n",
        "  else:\n",
        "    pass\n",
        "  params = {'view': 'proTeamSchedules_wl'}\n",
        "  r = requests.get(f'{base_url}seasons/{season}', params=params, cookies=auth_cookies)\n",
        "  json_object = json.dumps(r.json(), indent = 4)\n",
        "  with open(nfl_schedule_filename, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "  time.sleep(0.08)\n",
        "\n",
        "#Standings At End of Season\n",
        "  standings_filename = f\"standings_{season}.json\"\n",
        "\n",
        "  if os.path.exists(standings_filename):\n",
        "    os.remove(standings_filename )\n",
        "  else:\n",
        "    pass\n",
        "  r = requests.get(f'{endpoint}?view=mStandings', cookies=auth_cookies)\n",
        "  json_object = json.dumps(r.json(), indent = 4)\n",
        "  with open(standings_filename , \"w\") as outfile:\n",
        "     outfile.write(json_object)\n",
        "  time.sleep(0.08)\n",
        "\n",
        "# Player_card\n",
        "  player_card_filename = f\"player_card_{season}.json\"\n",
        "\n",
        "  if os.path.exists(player_card_filename):\n",
        "    os.remove(player_card_filename)\n",
        "  else:\n",
        "    pass\n",
        "  data ={'players': {'filterStatsForTopScoringPeriodIds':{'value': 16}}}\n",
        "  json_string = json.dumps(data)\n",
        "  params = {'view': 'kona_playercard'}\n",
        "  headers = {'x-fantasy-filter': json_string}\n",
        "  r = requests.get(endpoint, params=params, headers=headers, cookies=auth_cookies)\n",
        "  json_object = json.dumps(r.json(), indent = 4)\n",
        "  with open(player_card_filename, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "  time.sleep(0.08)\n",
        "\n",
        "# League info\n",
        "  league_filename = f\"league_{season}.json\"\n",
        "\n",
        "  if os.path.exists(league_filename):\n",
        "    os.remove(league_filename)\n",
        "  else:\n",
        "    pass\n",
        "  r = requests.get(f'{endpoint}?view=mSettings', cookies=auth_cookies)\n",
        "  json_object = json.dumps(r.json(), indent = 4)\n",
        "  with open(league_filename, \"w\") as outfile:\n",
        "     outfile.write(json_object)\n",
        "  time.sleep(0.08)\n",
        "\n",
        "# Teams - Fantasy Football (not nfl)\n",
        "  teams_filename = f\"teams_{season}.json\"\n",
        "\n",
        "  if os.path.exists(teams_filename):\n",
        "    os.remove(teams_filename)\n",
        "  else:\n",
        "    pass\n",
        "  r = requests.get(f'{endpoint}?&view=mTeam', cookies=auth_cookies)\n",
        "  json_object = json.dumps(r.json(), indent = 4)\n",
        "  with open(teams_filename, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "  time.sleep(0.08)\n",
        "\n",
        "# Players\n",
        "  player_filename = f\"players_{season}.json\"\n",
        "\n",
        "  if os.path.exists(player_filename):\n",
        "    os.remove(player_filename)\n",
        "  else:\n",
        "    pass\n",
        "  data = {'filterActive': {'value': True}}\n",
        "  json_string = json.dumps(data)\n",
        "  params = {'view': 'players_wl'}\n",
        "  headers = {'x-fantasy-filter': json_string}\n",
        "  r = requests.get(f'{endpoint}/players', params=params, headers=headers, cookies=auth_cookies)\n",
        "  json_object = json.dumps(r.json(), indent = 4)\n",
        "  with open(player_filename, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "  time.sleep(0.08)\n",
        "\n",
        "#Draft\n",
        "  draft_filename = f\"draft_{season}.json\"\n",
        "\n",
        "  if os.path.exists(draft_filename):\n",
        "    os.remove(draft_filename)\n",
        "  else:\n",
        "    pass\n",
        "  r = requests.get(f\"{endpoint}?&view=mDraftDetail\", cookies=auth_cookies)\n",
        "  json_object = json.dumps(r.json(), indent = 4)\n",
        "  with open(draft_filename, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "  time.sleep(0.08)\n",
        "\n",
        "# Free Agency (weekly)\n",
        "  data = {'players': {'filterStatus': {'value': ['FREEAGENT', 'WAIVERS']}, 'limit': 2000, 'sortPercOwned': {'sortAsc': False, 'sortPriority': 1}}}\n",
        "  json_string = json.dumps(data)\n",
        "  headers = {'x-fantasy-filter': json_string}\n",
        "  for week in range(1,18):\n",
        "\n",
        "    fa_filename = f\"free_agency_{season}_week_{week}.json\"\n",
        "\n",
        "    if os.path.exists(fa_filename):\n",
        "      os.remove(fa_filename)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    r = requests.get(f'{endpoint}?scoringPeriodId={week}&view=kona_player_info', headers=headers, cookies=auth_cookies)\n",
        "    json_object = json.dumps(r.json(), indent = 4)\n",
        "    with open(fa_filename, \"w\") as outfile:\n",
        "      outfile.write(json_object)\n",
        "    time.sleep(0.08)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P9GGFzamPkhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53b702f-6d18-4d60-f525-2d84b448c02f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting data for year: 2018\n",
            "Getting data for year: 2019\n",
            "Getting data for year: 2020\n",
            "Getting data for year: 2021\n",
            "Getting data for year: 2022\n",
            "Getting data for year: 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Raw JSON Files in GCP Storage Bucket\n",
        "\n",
        "\n",
        "This step isn't required, but can be helpful to store raw data in a known good state before applying all the transformations. We'll have to do a lot of work to make the raw json human readable... or at least fantasy football addict readable. This is a good way to reduce risk of having to repeat the whole extract if we mess up the files during the transforms."
      ],
      "metadata": {
        "id": "m6eO72lgIczL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.cloud import storage\n",
        "import uuid\n",
        "\n",
        "project_id = 'might-be-my-year'  # Replace with your actual project ID\n",
        "\n",
        "# Instantiates a client\n",
        "storage_client = storage.Client(project=project_id)\n",
        "random_uuid=str(uuid.uuid4())\n",
        "\n",
        "# The name for the new bucket\n",
        "bucket_name = f'fantasy-football-{league_id}-{random_uuid}'  # Replace with your bucket name\n",
        "\n",
        "# Creates the new bucket\n",
        "bucket = storage_client.create_bucket(bucket_name) # Call the create_bucket method to actually create the bucket\n",
        "print(f\"Bucket {bucket_name} created.\") # Add a confirmation message\n",
        "\n",
        "# Creates the new bucket\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "# Uploads files to the bucket\n",
        "for filename in os.listdir():\n",
        "    if filename.endswith(\".json\"):\n",
        "        blob = bucket.blob(filename)\n",
        "        blob.upload_from_filename(filename)\n",
        "        print(f\"File {filename} uploaded to {bucket_name}.\")\n"
      ],
      "metadata": {
        "id": "SN8mkXDtaBjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform Raw Data into Format and Schema for BigQuery"
      ],
      "metadata": {
        "id": "o8w5iFUvMSRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten File Format"
      ],
      "metadata": {
        "id": "ZruCZsYOM7em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test to make sure pandas can put the json into a dataframe\n",
        "df = pd.json_normalize(data, record_path=['teams'])\n",
        "df"
      ],
      "metadata": {
        "id": "_twQzSOJP_93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to Parquet"
      ],
      "metadata": {
        "id": "-0vWRUnrNNZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Draft Details\n",
        "\n",
        "draft_url= f'https://lm-api-reads.fantasy.espn.com/apis/v3/games/ffl/seasons/{season}/segments/0/leagues/{league_id}?view=mDraftDetail&view=mSettings&view=mTeam&view=modular&view=mNav'\n",
        "\n",
        "r = requests.get(draft_url, cookies={'swid': swid, 'espn_s2': espn_s2})\n",
        "if r.status_code == 200:\n",
        "\n",
        "  draft_data = r.json()\n",
        "  draft_data\n",
        "\n",
        "# Save the data to a JSON file\n",
        "  with open('draft_data.json', 'w') as f:\n",
        "        json.dump(draft_data, f, indent=0)  # Use indent for pretty printing\n",
        "  print('JSON file saved successfully.')\n",
        "else:\n",
        "  print('Request failed with status code:', r.status_code)\n",
        "\n",
        "  # Read the JSON file\n",
        "with open('draft_data.json', 'r') as f:\n",
        "    draft_data = json.load(f)\n",
        "\n",
        "draft_picks_df = pd.json_normalize(draft_data['draftDetail'], record_path=['picks'])\n",
        "draft_members_df = pd.json_normalize(draft_data, record_path=['members'])\n",
        "draft_settings_df = pd.json_normalize(draft_data['settings'])\n",
        "draft_status_df = pd.json_normalize(draft_data['status'])\n",
        "draft_teams_df = pd.json_normalize(draft_data, record_path=['teams'])\n",
        "\n",
        "# test to make sure dataframes are working\n",
        "draft_teams_df\n"
      ],
      "metadata": {
        "id": "9ty2xRL2SybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Parquet Files in GCP Storage Bucket\n",
        "\n",
        "Same process as before. Good checkpoint to save known good state.\n"
      ],
      "metadata": {
        "id": "yhMFO7HPNc3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data into BigQuery for Storage & Analysis"
      ],
      "metadata": {
        "id": "nBLt9WjlERlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data in BigQuery - This makes is easier to manually slice and dice"
      ],
      "metadata": {
        "id": "XSkBBVBOp5f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import pyarrow as pa\n",
        "from google.cloud.exceptions import NotFound\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# set project id again\n",
        "project_id = 'might-be-my-year'  # Replace with your actual project ID\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "bq_client = bigquery.Client(project=project_id)\n",
        "\n",
        "# TODO(developer): Set dataset_id to the ID of the dataset to create.\n",
        "dataset_id = \"{}.fantasy_football\".format(bq_client.project)\n",
        "\n",
        "# Construct a full Dataset object to send to the API.\n",
        "dataset = bigquery.Dataset(dataset_id)\n",
        "\n",
        "#Specify the geographic location where the dataset should reside.\n",
        "dataset.location = \"US\"\n",
        "\n",
        "# Check if the dataset exists\n",
        "try:\n",
        "    bq_client.get_dataset(dataset_id)  # Make an API request.\n",
        "    print(f\"Dataset {dataset_id} already exists.\")\n",
        "except NotFound:\n",
        "    print(f\"Dataset {dataset_id} does not exist. Creating...\")\n",
        "# Send the dataset to the API for creation, with an explicit timeout.\n",
        "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
        "# exists within the project.\n",
        "    dataset = bq_client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "    print(\"Created dataset {}.{}\".format(bq_client.project, dataset.dataset_id))\n",
        "\n",
        "# create lists of all dataframes with coooresponding table names\n",
        "all_dfs = [draft_picks_df, draft_members_df, draft_settings_df, draft_status_df, draft_teams_df]\n",
        "table_names = ['draft_picks', 'draft_members', 'draft_settings', 'draft_status', 'draft_teams']\n",
        "\n",
        "for df, table_name in zip(all_dfs, table_names):\n",
        "\n",
        "# Replace periods in column names with underscores\n",
        "  df.columns = df.columns.str.replace('.', '_')  # Replace '.' with '_' in column names because thats what BQ needs\n",
        "# Convert the DataFrame to a Parquet file\n",
        "  table = pa.Table.from_pandas(df)\n",
        "  pq.write_table(table, f'{table_name}.parquet')\n",
        "\n",
        "# TODO(developer): Set table_id to the ID of the table to create.\n",
        "  table_id = f'{dataset_id}.{table_name}_{season}'\n",
        "\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "      autodetect=True, source_format=bigquery.SourceFormat.PARQUET\n",
        "  )\n",
        "\n",
        "# open parquet file -> load the table from parquet file into bq table\n",
        "  with open(f'{table_name}.parquet', \"rb\") as source_file:\n",
        "      job = bq_client.load_table_from_file(\n",
        "          source_file, table_id, job_config=job_config\n",
        "      )\n",
        "\n",
        "# Wait for the job to complete\n",
        "  job.result()\n",
        "\n",
        "  print(\"Loaded {} rows and {} columns to {}\".format(job.output_rows, len(df.columns), table_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TJdAsHVp5KJ",
        "outputId": "ff95f536-1c76-462b-ec8f-c510fa73a4bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset might-be-my-year.fantasy_football already exists.\n",
            "Loaded 180 rows and 14 columns to might-be-my-year.fantasy_football.draft_picks_2023\n",
            "Loaded 12 rows and 7 columns to might-be-my-year.fantasy_football.draft_members_2023\n",
            "Loaded 1 rows and 131 columns to might-be-my-year.fantasy_football.draft_settings_2023\n",
            "Loaded 1 rows and 56 columns to might-be-my-year.fantasy_football.draft_status_2023\n",
            "Loaded 12 rows and 125 columns to might-be-my-year.fantasy_football.draft_teams_2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis & Modeling"
      ],
      "metadata": {
        "id": "VjaxusUOOFlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze Data\n",
        "\n",
        "Goal: Plot \"Winners\" and \"Losers\" based off of draft day auction price vs. production.\n",
        "\n",
        "should be somthing like avg auction value vs price paid for on draft day vs inferred auction value based on end of year performance\n",
        "\n",
        "plot 1: avg auction value vs price paid for on draft day. + difference shows league values that player/position more, - difference league values that player/position less\n",
        "\n",
        "plot 2: avg auction value vs inferred auction value based on end of the year performance. + difference = player outperfomed expectations, - difference = player underperformed expectations\n",
        "\n",
        "Plot 3: differnece in plot 1 vs difference in plot 2. quadrant plot: q1 =  players who under performed and the league overvalued. q2 = players who out performed and the league over valued. q3 = players who outperformed and the league undervalued. q4 =  players that underperformed and the league undervalued\n"
      ],
      "metadata": {
        "id": "44cq2X_pHGnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "TtDLy-p9J2Kr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}